%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Example: Project Report
%
% Source: http://www.howtotex.com
%
% Feel free to distribute this example, but please keep the referral
% to howtotex.com
% Date: March 2011 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Edit the title below to update the display in My Documents
%\title{Project Report}
%
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}


%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{School of random department names} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge CSE 221 Final Project \\
        \huge       Draft 1         \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont 								\normalsize
        Patrick Bird\\[-3pt]		\normalsize
        patbird@gmail.com\\           \normalsize
        \today
}
\date{}


%%% Begin document
\begin{document}
\maketitle
\section{Introduction}
Over the last decade, an increasing number of services are transitioning to the cloud to leverage some of its power.  Pre-configured virtual machines and unlimited object storage attract many a company.  Many of these services attract companies who do not want to invest time and money into supporting infrastructure that is not in their area of specialty.

The area of big data is also an interest to many.  Many organizations and instutions are amassing large datasets that is challenging on one hand since there is much potential to learn new connections between data that wasn't known before.  But it is also crippling from not knowing where to start analyzing and drawing connections.

With both cloud technology and big data, the infrastructure is crtical for maintaining a robust platform to explore these two areas of technology.  The institution where I work in is involved in these two areas and that is what has motivated me to understand and characterize a system.

The current project that I work on, basespace.com, leverages Amazon AWS services such as EC2, S3, SQS, and more.  And since our product is mainly the analysis of genomic data, the need for high throughput of both data, compute, and parallelism is needed.  Many of our analyses can take several hours to compute.  This timespan may seem acceptable, but as genomics and the clinical lab converge, speed will be critical for diagnosing health issues.  Any savings in time could, without sounding too cliche, save a life.

Focusing on cloud computing, I wanted to characterize a local, high-performance bare-metal compute node.  Although outside the scope of this paper, I would eventually like to extend the study to actual VMs on these compute nodes and to compare their performance with the bare metal.  For instance, how does an 8 VCPU VM with 64 GB of memory compare with its bare-metal constituents.  What would be the cost of virtualization?

Since this study involves high performance time measurement, I wanted to get as close as I could to the hardware without writing the whole thing in assembly.  Not to mention, the node runs CentOS linux, and knowing that GCC and the standard POSIX C headers were readily available, no other languages were really considered.  No special compiler options were really used either.  The only options I needed to provide were to link in math libraries for calculating the square root and for using pthreads.

So far, I have invested about 25 hours into this project thus far.  I alone have done all the work in both research, code implementation and the production of this paper.

\section{Machine Description}

The system has two processors, with each having six cores that are hyperthreaded.  This information was called from /proc/cpuinfo and matched with the information from Intel's website.  The processors are Intel Xeon E5-2620 that run at 2000.053 MHz to be exact.  There are three levels of cache - 32 KB of L1 each for data and instructions, 256 KB of L2 (both), and 15360 KB of L3 (both).  

The memory size is 128 GB of DDR3 RAM.  The potential speed is 1600 MHZ, however, the configurable clock speed is set to 1333 MHz.  The processor is setup so that it can utilize four memory channels with the RAM.  The more channels it has, the greater opportunity the processor has for read and write parallelism.  This information was found by the dmidecode command.

The network card is full duplex gigabit ethernet.

The operating system is 64-bit CentOS v6.4, kernel version v2.6.32-358.11.1.

\section{Operations}
This section will describe various operations that I estimated performance, how I came to those estimates, and how I performed measurements to measure these operations.  To begin with, this system is not a single RISC processor.  Unfortunatley for my current purposes, but to my computer's delight, there is not a one to one mapping between instructions and clock cycles.  At least for the first few operations, I couldn't merely just count up the instructions and add a little for overhead.

As I mentioned in Section 2, these processsors have multi-cores three levels of cache, around a 17-20 stage pipeline, an interprocessor communication mechanism.  Needless to say, estimating any sort of operation is difficult without an exhaustive knowledge of the internals of the processor and the system.

Since there are so many variables in the system, I wanted to set out and eliminate as many as I could before I started to measure.  For one, I wanted to bind this measurement process to a particular processor.  If my process was interrupted and switched out to a different one, I wanted to ensure that the clock that I read every time was from the same processor.

Thus, one of the first things that my process does upon startup is to set processor affinity.  There is a POSIX routine for this called \textit{sched\_setaffinity}.  When this routine is run at the beginning of my process, the process binds itself to the first processor.  With this binding, at least my clock will not vary from jumping to different CPUs.

My next goal was to elevate the priority of the process to limit context switching as much as possible.  Obviously, I wouldn't have full control over how much CPU bandwidth I could use.  Again, there is a POSIX procedure for setting the priority - \textit{setpriority}.  This process is elevated to the minimum nice value, which is the highest priority.  At this point in time, it is -20.

Both of these decisions did not come easily.  The main drawback was that the typical use case would be eliminated, at least for the time being.  When performing measurements, it is good to know what the measurement would be for typical scenearios instead of highly optimized ones.  However, what caused me to decide to bind to a processor and elevate the process' priority was again the effort to eliminate as many variables as I could.  My reasoning was that these variables can always be reintroduced later if they needed to be measured.

All the measurements are in clock cycles.  However, a simple formula exists to calculate the time that operation took:

INSERT HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

\subsection{Measurement Overhead}

How time is measured for performance metrics such as this paper is one of the more fundamental mechanisms.  Every measurement will be built on how time is measured.  And since the system is incredibly complex, measuring time is a difficult process.  One of the first decisions that needed to be made was whether to count clock cycles or to measure time through some sort of native timestamp.

At least to begin with, some of the operations were quite small which warranted the granularity of the clock cycle.  Any library call that would get the calendar time would need to have some extra code to translate the real time clock to a data structure.  This translation would add yet another cost to understanding the true overhead of the system.

Thus, \textbf{RDTSC} seemed to be the operation to go.  It is one instruction on this chipset, which would read the real time clock of the CPU.  And it is not a privleged call, so there is no overhead in reading it.  However, as I read more about the instruction and the CPU, I understood that the CPU is out of order processor.  Thus, there is no guaranteee when a certain instruction will execute.  To put it in other words, one cannot just look at the assembly and trust that the processor will execute it sequentially.  This processor has almost twenty stages in its pipeline.

Thus, there needed to be a way to serialize the calls, or create an instruction barrier.  \textbf{CPUID} seemed to be a favorite instruction to call right before \textbf{RDTSC} since it is a serializing call.  Unfortunately, this instruction comes with somewhat of a performance penalty.  Thankfully, I came across \textbf{RDTSCP}, which is a serializing \textbf{RDTSC} call.

\subsubsection{Estimation}

After the small routine to read the clock is loaded into cache, there is not much hardware interaction other than the CPU itself.  Since this is the case, a hardware estimation includes examining the software routine for performance.  There are fifteen assembly instructions in the \textit{GetRdtscp()} procedure with another four instructions for calling and returning.  In all, there about twenty instructions.  Accounting for 20% overhead, my hardware estimation is 24 clock cycles.

The operating system does not add much overhead in this operation.  After the instructions are loaded from memory, the instructions will most likely stay in cache.  Thus, another 20% overhead will be accounted for random context switching.  Thus, the overall estimation time is around 30 clock cycles.

After running the experiment a 1000 times, I get the following results:

\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    Hardware Estimate & Software Estimate & Total Estimate & Measurement \\ \hline
    24 cycles & 5 cycles & 30 cyles & 35 cyles \\ 
    \hline
    \end{tabular}
\end{center}

\begin{center}
    \begin{tabular}{ | l | l | l | l | p{5cm} |}
    \hline
    First iteration & Min & Max & Mean & Std Dev & Sample Count \\ \hline
    48 cycles & 30 cycles & 41 cyles @ 23rd iteration & 35.43 cyles & 2.85 cycles & 1000 iterations \\ 
    \hline
    \end{tabular}
\end{center}


%%% End document
\end{document}