%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Example: Project Report
%
% Source: http://www.howtotex.com
%
% Feel free to distribute this example, but please keep the referral
% to howtotex.com
% Date: March 2011 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Edit the title below to update the display in My Documents
%\title{Project Report}
%
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}                                                         % English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}  
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}   
\usepackage{url}
\usepackage{pstricks-add}
\usepackage{pgfplots}
\usepackage{listings}

%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}


%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}                                            % No page header
\fancyfoot[L]{}                                         % Empty 
\fancyfoot[C]{}                                         % Empty
\fancyfoot[R]{\thepage}                                 % Pagenumbering
\renewcommand{\headrulewidth}{0pt}          % Remove header underlines
\renewcommand{\footrulewidth}{0pt}              % Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}        % Equationnumbering: section.eq#
\numberwithin{figure}{section}          % Figurenumbering: section.fig#
\numberwithin{table}{section}               % Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}}     % Horizontal rule

\title{
        %\vspace{-1in}  
        \usefont{OT1}{bch}{b}{n}
        \normalfont \normalsize \textsc{} \\ [25pt]
        \horrule{0.5pt} \\[0.4cm]
        \huge CSE 221 Final Project \\
        \huge       Final Report         \\
        \horrule{2pt} \\[0.5cm]
}
\author{
        \normalfont                                 \normalsize
        Patrick Bird\\[-3pt]        \normalsize
        patbird@gmail.com\\           \normalsize
        \today
}
\date{}


%%% Begin document
\begin{document}
\maketitle
\section{Introduction}
Over the last decade, an increasing number of services are transitioning to the cloud to leverage some of its power.  Pre-configured virtual machines and unlimited object storage attract many an organization.  Many of these services attract companies who do not want to invest time and money into supporting infrastructure that is not in their area of specialty.

The area of big data is also an interest to many.  Many organizations and instutions are amassing large datasets that is challenging on one hand and frightening on another.  It's exciting since there is much potential to learn new connections between data that wasn't known before.  But it is also crippling from not knowing where to start analyzing and drawing connections.

With both cloud technology and big data as dominant technological forces, the infrastructure is crtical for maintaining a robust platform to explore these two areas of technology.  The institution where I work in is involved in these two areas and that is what has motivated me to understand and characterize a portion of that system.

The current work project that I work on, basespace.com, leverages Amazon AWS services such as EC2, S3, SQS, and more.  And since our product is mainly the analysis of genomic data, the need for high throughput of data, compute, and parallelism is needed.  Many of our data analyses can take several hours to compute.  This timespan may seem acceptable, but as genomics and the clinical lab converge, speed will be critical for diagnosing health issues.

Focusing on cloud computing, I wanted to characterize a local, high-performance bare-metal compute node.  Although outside the scope of this paper, I would eventually like to extend the study to actual VMs on these compute nodes and to compare their performance with the bare metal.  For instance, how does an 8 VCPU virtual machine with 64 GB of memory compare with its bare-metal constituents.  What would be the cost of virtualization?

Since this study involves high performance time measurement, I wanted to get as close as I could to the hardware without writing the whole thing in assembly.  Not to mention, the node runs CentOS linux, and knowing that GCC and the standard POSIX C headers were readily available, no other languages were really considered.  No special compiler options were really used either.  The only options I needed to provide were to link in math libraries for calculating the square root and for using pthreads.

So far, I have invested about 30 hours into this project thus far.  I alone have done all the work in both research, code implementation, and the production of this paper.

\section{Machine Description}

The system has two processors, with each having six cores that are hyperthreaded.  This information was called from /proc/cpuinfo and matched with the information from Intel's website.  The processors are Intel Xeon E5-2620 that run at 2000.053 MHz to be exact.  There are three levels of cache - 32 KB of L1 each for data and instructions, 256 KB of L2 (both), and 15,360 KB of L3 (both).  The instruction pipeline is anywhere from 17 to 20 stages.

The memory size is 128 GB of DDR3 RAM.  The potential speed is 1600 MHZ, however due to the fact that the processor can at max handle 1333 MHz, the configurable clock speed is set to 1333 MHz.  The processor is setup so that it can utilize four memory channels with the RAM.  The more channels it has, the greater opportunity the processor has for read and write parallelism.  This information was found by the dmidecode command.

The hard disk is a 1 TB hard disk drive at 7200 RPM.  The network card is full duplex gigabit ethernet.  The operating system is 64-bit CentOS v6.4, kernel version v2.6.32-358.11.1.

Here is a summary of the system:

\begin{center}
\begin{tabular}{ |l|l| }
  \hline
  \multicolumn{2}{|c|}{System Specifications} \\
  \hline
  \# of processors & 2 \\
  cores per processor & 6 \\
  processor manufacturer & Intel Xeon E5-2620 \@ 2000.053 MHz \\
  L1 Size (Data) & 32 KB \\
  L1 Size (Instructions) & 32 KB \\
  L2 Size & 256 KB \\
  L3 Size & 15,360 KB \\
  Instruction Pipeline & 17-20 stages \\
  RAM & 128 GB DDR3 RAM \@ 1333 MHz \\
  \# memory channels & 4 \\
  Hard Disk & 7200 RPM (1 TB) \\
  Network & Full Duplex Gigabit \\
  Operating System & 64-bit CentOS v6.4, kernel v.2.6.32-358.11.1 \\
  \hline
\end{tabular}
\end{center}

\section{Operations}

This section will describe various machine operations, how I estimated performance on them, and how I performed measurements to measure these operations and their corresponding results.  To begin with, this system is not a single RISC processor.  Unfortunatley for my current purposes, but to my computer's delight, there is not a one to one mapping between instructions and clock cycles.  At least for the first few operations, I couldn't merely just count up the instructions and add a little for overhead.

As I mentioned in Section 2, these processsors have multi-cores three levels of cache, and a 17-20 stage pipeline, with an interprocessor communication mechanism.  Needless to say, estimating any sort of operation is difficult without an exhaustive knowledge of the internals of the processor and the system.  And even then, the estimation would still only be a ballpark figure.

Since there are so many variables in the system, I wanted to set out and eliminate as many as I could before I started to measure.  For one, I wanted to bind this measurement process to a particular processor.  If my process was interrupted and switched out to a different one, I wanted to ensure that the clock that I read every time was from the same processor.

Thus, one of the first things that my process does upon startup is to set processor affinity.  There is a POSIX routine for this called \textit{sched\_setaffinity}.  When this routine is run at the beginning of my process, the process binds itself to the first processor.  With this binding, at least my clock will not vary from jumping to different CPUs.

My next goal was to elevate the priority of the process to limit context switching as much as possible.  Obviously, I wouldn't have full control over how much CPU bandwidth I could use.  Again, there is a POSIX procedure for setting the priority - \textit{setpriority}.  This process is elevated to the minimum nice value, which is the highest priority.  At this point in time, it is -20.

Both of these decisions did not come easily.  The main drawback was that the typical use case would be eliminated, at least for the time being.  When performing measurements, it is good to know what the measurement would be for typical scenearios instead of highly optimized ones.  However, what caused me to decide to bind to a processor and elevate the process' priority was again the effort to eliminate as many variables as I could.  My reasoning was that these variables can always be reintroduced later if they needed to be measured.

All the measurements are in clock cycles.  However, a simple formula exists to calculate the time that operation took:

\begin{gather}
time(s)=cycles/2,000,000,000 Hz
\end{gather}

\subsection{Measurement Overhead}

How time is measured for performance metrics such as this paper is one of the more fundamental mechanisms.  Every measurement will be built on how time is measured.  And since the system is incredibly complex, measuring time is a difficult process.  One of the first decisions that needed to be made was whether to count clock cycles or to measure time through some sort of native timestamp.

At least to begin with, some of the operations were quite small which warranted the granularity of the clock cycle.  Any library call that would get the calendar time would need to have some extra code to translate the real time clock to a data structure.  This translation would add yet another cost to understanding the true overhead of the system.

Thus, \textbf{RDTSC} seemed to be the operation to go.  It is one instruction on this chipset, which would read the real time clock of the CPU.  And it is not a privleged call, so there is no overhead in reading it.  However, as I read more about the instruction and the CPU, I understood that the CPU is out of order processor.  Thus, there is no guaranteee when a certain instruction will execute.  To put it in other words, one cannot just look at the assembly and trust that the processor will execute it sequentially.  This processor has almost twenty stages in its pipeline.

Thus, there needed to be a way to serialize the calls, or create an instruction barrier.  \textbf{CPUID} seemed to be a favorite instruction to call right before \textbf{RDTSC} since it is a serializing call.  Unfortunately, this instruction comes with somewhat of a performance penalty.  Thankfully, I came across \textbf{RDTSCP}, which is a serializing \textbf{RDTSC} call.

\subsubsection{Estimation}

After the small routine to read the clock is loaded into cache, there is not much hardware interaction other than the CPU itself.  Since this is the case, a hardware estimation includes examining the software routine for performance.  There are fifteen assembly instructions in the \textit{GetRdtscp()} procedure with another four instructions for calling and returning.  In all, there about twenty instructions.  Accounting for 20\% overhead, my hardware estimation is 24 clock cycles.

The operating system does not add much overhead in this operation.  After the instructions are loaded from memory, the instructions will most likely stay in cache.  Thus, another 20\% overhead will be accounted for random context switching.  Thus, the overall estimation time is around 30 clock cycles (15 ns).

\subsubsection{Measurement}

After running the experiment a 1000 times, I get the following results:

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Hardware Estimate & Software Estimate & Total Estimate & Measurement \\ \hline
    24 cycles & 5 cycles & 30 cycles & 35 cycles (18 ns) \\ 
    \hline
    \end{tabular}
\end{center}

My estimation was actually not as bad as I expected it to be.  My estimation was faster than what the average speed of the routine was, but close enough.  In fact, my estimation matched the minimal value that I observed over 1000 iterations.  Part of the reason why I think my estimate was fairly good was that there was not much happening here as far as the system goes.  There is no outside interface with peripheral devices or memory for that matter.

The memory issue is interesting because for the most part, my performance only took a hit during the first iteration.  The CPU had to fetch the procedure from memory and load it into its cache.  After the routine was in cache, the performance increased significantly.

As one can observe by the next grid, there is a tight range between the min and the max.  There is only a difference of 11 clock cycles with a standard deviation of 2.68 clock cycles.  The max in the table lists the max other than the first data point.  I intentionally did this for two reasons.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    No. of Samples & First Iteration & Min & Max (excluding 1st) & Mean & Standard Deviation \\ \hline
    1000 iterations & 53 cycles & 30 cycles & 41 cyles & 35.0 cyles & 2.68 cycles \\ 
    \hline
    \end{tabular}
\end{center}

For one, in all my measurements, I want to observe the cost of the CPU fetching the routine from memory if it has not already done so.  I also wanted to know after that, what is the next maximum value.  I followed this process for each of the operations.

\subsection{Loop Overhead}

\subsubsection{Estimation}

Adding a loop construct to the measurement routine does not introduce a heavy burden.  In reality, it is just a few more instructions to add to the measurement routine.  And since it is a loop, there is pointer arithemetic that needs to happen to store the clock value.  Again there is no significant operating system cost to account for, so the entire extra cost for a loop routine will fall in the hardware domain.  Using the average cost from the RDTSCP routine, I estimate that the cost for a loop will be an extra ten cycles based upon the additional hardware instructions or 38 clock cycles (19 ns).

\subsubsection{Measurement}

After running the experiment a 1000 times, I observe the following results:

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Hardware Estimate & Software Estimate & Total Estimate & Measurement \\ \hline
    5 cycles & 5 cycles & 10 cycles & 12 cycles (6 ns) \\ 
    \hline
    \end{tabular}
\end{center}

The estimate was actually very close to the actual answer which is very pleasing.  Though, trying to estimate this routine from scratch would have been much more difficult.  Since I had the convienence of the previous metric, my estimate was able to be close to the measured value.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    No. of Samples & First Iteration & Min & Max (excluding 1st) & Mean & Standard Deviation \\ \hline
    1000 iterations & 45 cycles & 45 cycles & 371 cycles & 35.0 cycles & 2.68 cycles \\ 
    \hline
    \end{tabular}
\end{center}

\subsection{Procedure}

\subsubsection{Estimation}

The cost of calling a procedure will most likely be on par with the loop construct.  In the loop construct, there is a branch, an update to a register value, and pointer arithmetic for both the lower and upper 32 bits of the clock value.  For a procedure, it will be pushing and popping registers off the stack.  Obviously, the greater number of arguments, the greater number of registers that need to be pushed and popped.

For zero arguments, there is almost no cost to calling a function other than pushing the return address and the call instruction itself.  For one argument, a register needs to be pushed, and the stack pointer needs to be adjusted.   Also, there is the call instruction. Then, for every variable, an additional register is pushed onto the stack.  Thus, we can use the equation

\begin{align}
clocks& =RDTSCP + 2 + 2n\\
clocks& =2n + 37
\end{align}

Here is a graph of clock cycles against number of arguments.

\begin{center}
\begin{tikzpicture}
  \begin{axis}[ 
    xmin=-1, xmax=10,
    ymin=30, ymax=50,
    xlabel=$num of arguments$,
    ylabel={$clock cycles$},
    axis on top=true
  ] 
    \addplot {2*x + 37}; 
  \end{axis}
\end{tikzpicture}
\end{center}

In the C routines themselves, I ensured that the arguments were being referenced so the compiler would not try to optimize them away.

\subsubsection{Measurement}

Again, stepping on the shoulders of the RDTSCP measurement has been beneficial.  The measurements are within the bounds of my estimation.

\begin{center}
    \begin{tabular}{ | l| l | l | l | l |}
    \hline
    Argument Count & Hardware Estimate & Software Estimate & Total Estimate & Measurement \\ \hline
    0 & 37 cycles & 5 cycles & 43 cycles & 37 cycles (24 ns) \\ \hline
    1 & 39 cycles & 5 cycles & 44 cycles & 38 cycles (24 ns) \\  \hline
    2 & 41 cycles & 5 cycles & 46 cycles & 40 cycles (24 ns) \\  \hline
    3 & 43 cycles & 5 cycles & 48 cycles & 40 cycles (24 ns) \\  \hline
    4 & 45 cycles & 5 cycles & 50 cycles & 41 cycles (24 ns) \\  \hline
    5 & 47 cycles & 5 cycles & 52 cycles & 40 cycles (24 ns) \\  \hline
    6 & 49 cycles & 5 cycles & 54 cycles & 42 cycles (24 ns) \\ 
    \hline
    \end{tabular}
\end{center}

It is interesting to note that functions with two, three, four, and six arguments had the same average cost of 40 clock cycles.  Not to mention that all eight enumerations of functions are within five clock cyles of each other.  So in some ways there is a linear relationship between additional arguments and the time cost.  The linear relationshiop can be explained because each arguments is pushed on the stack when the function is called.  Thus, every argument will be a couple of instructions.  However, the cost is neglible.

The various metrics for the different function calls are enumerated here:

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l | l |}
    \hline
    Arg COunt & No. of Samples & First Iteration & Min & Max & Mean & Std Dev \\ \hline
   0 & 1000 iterations & 89 cycles & 30 cycles & 44 cycles & 37.0 cycles & 4.14 cycles \\ 
    1 &    1000 iterations & 65 cycles & 30 cycles & 44 cycles & 38.0 cycles & 3.2 cycles \\ 
     2 &       1000 iterations & 142 cycles & 33 cycles & 44 cycles & 41.0 cycles & 4.40 cycles \\ 
       3 &         1000 iterations & 68 cycles & 33 cycles & 47 cycles & 40.4 cycles & 3.11 cycles \\ 
        4 &            1000 iterations & 137 cycles & 33 cycles & 47 cycles & 40.9 cycles & 4.23 cycles \\ 
          5 &              1000 iterations & 145 cycles & 36 cycles & 47 cycles & 41.3 cycles & 4.25 cycles \\ 
             6 &               1000 iterations & 68 cycles & 36 cycles & 181 cycles & 40.78 cycles & 5.87 cycles \\ 
              7 &                  1000 iterations & 68 cycles & 36 cycles & 181 cycles & 42.64 cycles & 2.81 cycles \\ 
    \hline
    \end{tabular}
\end{center}

\subsection{System Call}

\subsubsection{Estimation}

For a system call, linux needs to trap into the kernel. Thus, there is a heavier cost for a system call as opposed to a user call.  The same registers are pushed onto the stack as a user call.  But there also needs to be some sort of interrupt that happens and an elevation of privleges.  After the user function makes a system call,  the kernel traps into its own routine and takes over.  The kernel then figures out which system call to route to and saves all relevant user state so when it is done, it can transfer control flow back to the user process.  Within the system call itself will copy any releveant user data to its own address space and perform its operation.  After the work is finished, the kernel transfers control flow back to the user process.

The system call is much more expensive than a normal function call because of the elevation of privleges.  Not to mention, there is more copying of data from user space to kernel space and back again.  And there is more state to save.

In the case of a system call, there is more overhead in the operating system than hardware.  As far as hardware is concerned, the CPU will most likely have to fetch the procedure from kernel's address space when it needs to be loaded.  Since system calls happen fairly often, a lot of the instruction data will most likely already be in cache.  But this cost is very similar to a procedure in user space.  It is just in a different place in memory.  Thus, the hardware cost is the base from a user function call - 37 clock cycles.

However, there is a greater cost to the OS here.  In my case, I will measure \textit{time()}.  There are at least seven different stages enumerated for this system call.  For each stage, I added an estimate.



\begin{tabular}{ |l|l| }
  \hline
  \multicolumn{2}{|c|}{OS Cost} \\
  \hline
  10 cycles & User state is saved \\
  20 cycles& Elevation of privleges \\
  10 cycles& System function called \\
  10 cycles& Any user data that is needed is copied into kernel space with correct privleges \\
  20 cycles& System call executes \\
  10 cycles& Execution is restored to user \\
  \hline
  80 cycles& total \\
  \hline
\end{tabular}


In all, I estimate about 100 clock cycles will happen just for the operating system part of the system call.  Adding to the hardware cost, I estimate that a system call will take about 117 clock cycles.

\subsubsection{Measurement}

The first time that this fuction was called, it took 3863 clock cycles.  On average, it was 147 clock cycles, about double what I expected.  This sytem call probably took a bit more time than I had expected because of some of the time logic that I did not account for.  Also, the crossing of boundaries is also probably more expensive than it was expected.

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Hardware Estimate & Software Estimate & Total Estimate & Measurement \\ \hline
    37 cycles & 100 cycles & 137 cycles & 147 cycles (74 ns) \\ 
    \hline
    \end{tabular}
\end{center}

The following statistical metrics are here.  Notice the cost of the initial call.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    No. of Samples & First Iteration & Min & Max (excluding 1st) & Mean & Standard Deviation \\ \hline
    1000 iterations & 2427 cycles & 36 cycles & 83 cycles & 42.41 cycles & 75.51 cycles \\ 
    \hline
    \end{tabular}
\end{center}


\subsection{Task Creation}

\subsubsection{Estimation}

For task creation, there are two main different implementations to take advantage of in linux - processes and pthreads.  The former is invoked using \textit{fork()} where a new process will be created by its parent.  And the two will continue to share memory until a copy-on-write occurs.  This happens whenever one of the processes writes to a shared variable.  Now each process will have its own copy.

A pthread on the other hand is a primitive that allows for different threads to share the same memory space.  Linux provides several primitives for the pthreads to synchronize around.

To create a pthread, one uses the \textit{pthread\_create()} function.  Here, there are two threads that share the same memory.  Synchronization is done with atomic shared memory objects like mutexs and condition variables.

The hardware cost for forking a process will mostly be driven by any memory that needs to be copied from the parent to the child.  And it also involves updating any of the parent's address space.  Thus, the cost of creating a new address space for the child is the most significant cost.  Updating several of pages might take a good tens of thousands of clock cycles since the TLBs and cache need to be updated and the permissions need to be copied appropriately.  The cost of hardware is estimated to be 50,000 clock cycles.

From an operating system perspective, the kernel needs to create the process, assign it a new ID, and initiate the copying of any data.  After copying the data, it needs to initiate any updates in the parent.  This management could take a bit of time especially if it is blocked by hardware.  The relative cost of these operations is set at 20000 clock cycles.  In total, I estimate about 70000 clock cycles.

As far as pthreads are concerned, the cost will probably not be as high since memory does not need to be copied over.  Thus, the hardware cost is probably around 10000, just in case any new memory needs to be referenced.  There is not much hardware cost since everything happens more at the kernel level.  The OS will handle the creation of the thread, checking its arguments are valid, and register a new thread at the kernel level.  Since the hardware cost is somewhat neglibile, the kernel cost will be about 10000 clock cycles. 

\subsubsection{Measurement}

Again, the estimates were off by a good bit for processes.  The estimate for threads were a little closer.  This discrepency has probably to do with understanding fork at the hardware level and what all that needs to be copied.  For threads, most of that is managed in software so the cost can be understood a little more.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Type & Hardware Estimate & Software Estimate & Total Estimate & Average Measurement \\      \hline
    Process & 50,000 cycles & 20,000 cycles & 70,000 cyles & 277,758 cyles (139 us) \\ 
    Thread & 10,000 cycle & 10,000 cycles & 20,000 cycles & 35,443 cycles (18 us) \\
    \hline
    \end{tabular}
\end{center}

The various statistical metrics for these measurements can be found below.  One aspect to notice is that there is a great degree of variance for these measurements.  This fact leads me to believe that memory and disk access has much to play in the cost of creating processes and threads.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    No. of Samples & 1st Iteration & Min & Max & Mean & Std Dev \\ \hline
    1000 iterations & 182,993 cycles & 83,727 cycles & 41,948,421 cyles & 277,758 cyles & 2,184,639 cycles \\ 
    1000 iterations & 87111 cycles & 19,804 cycles & 5,901,437 cyles & 35,443 cyles & 197,039 cycles \\ 
    \hline
    \end{tabular}
\end{center}

\subsection{Context Switching}

\subsubsection{Estimation}

The cost for a context switch involves swapping one process or thread out for another.  For processes, the OS needs to manage the synchronization of the processes.  Each OS has its own algorithm for doing so.  Priority and equity rule in this domain.  Just like we saw in the last measurement, the hardware cost for context switching will most likely be driven by the memory/disk cost again.  For instance, if a thread or a process needs to be swapped out, the instructions for the process need to be loaded into the CPU.  If it is a new task, it will most likely have to be retrieved from main memory.

Here, I estimate that thread context switching will be more streamlined than processes because of how threads share memory with one another.  My implementation for measuring the signalling was as follows.  STDIN and STDOUT are opened at the beginning of the  \textit{MeasureForkContextSwitch()} function.  The current process is then forked.  The parent process closes STDIN and call \textit{pause()} to wait for a user signal from its child process.  The child process on the other hand, when started, will close STDOUT, retrieve the timestamp and signal to its parent process.  After signalling, it writes the timestamp to its parent through the pipe.

My implementation for measuring pthread context switching is as follows.  A pthread is created by the main thread.  After creating the thread, the main thread waits on a condition variable.  When the pthread starts, it reads the time stamp value and signals the condition variable.  In the pthread's exit function, it returns the timestamp.  The main thread wakes up when its condition variable and then reads its timestamp.  Afterwards, it joins the other thread and calculates the difference.

The hardware estimate for processes depends on the memory involved and what needs to be swapped in and out.  Threads will be able to leverage more shared memory and not have to swap in and out as often as threads will.  Thus, the hardware estimate for process context switching will most likely be lower than the previous measurement since some of the data will probably be cached.  Thus, my estimate is half of the previous measurement's average which is about 140,000 clock cycles.  The operating system measurement is neglible compared to the hardware.  Thus, the OS estimate will be 10,000 clock cycles.  The total is 150,000 clock cycles.

For threads, the estimate will be lower since it leverages shared memory.  The average from the last measurement is about 35,000 clock cycles.  Thus, I will halve this one as well for memory hardware overhead.  The operating system is not as drastic.  It is involved in managing the condition variable and mutex.  Also, it is responsible for joining the two threads.  Not to mention, the operating system needs to swap one thread into its thread manager and swap the other one out.  The logic involved here might take 10,000 clock cycles.  In all, I estimate that the context switch will take about 45,000 clock cycles.

\subsubsection{Measurement}

The context switching was faster in both cases.  This discrepency is probably due to the fact that the processor was able to cache more of the instructions and data in cache than estimated, which in turn led to less swapping.  Also, the thread management overhead could be more efficient than previously estimated.  The thread context switching is actually extremely efficient compared to the process switching.  Again, this is mainly due to the fact that the threads share the same memory space.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Type & Hardware Estimate & Software Estimate & Total Estimate & Average Measurement \\      \hline
    Process & 140,000 cycles & 10,000 cycles & 150,000 cyles & 82,674 cycles (41 us) \\ 
    Thread & 35,000 cycle & 10,000 cycles & 45,000 cycles & 3114 cycles (1.6 us) \\
    \hline
    \end{tabular}
\end{center}

The statistical metrics for the various context switching are as follows.  Again, because memory swapping is involved and the TLB needs to be flushed whenever there is a context switch, there is huge variance in the measurements.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    No. of Samples & First Iteration & Min & Max & Mean & Std Dev \\ \hline
    1000 iterations & 4,878,939 cycles & 69,176 cycles & 165,570 cyles & 82,674 cyles & 151,883  cycles \\ 
    1000 iterations & 14,667 cycles & 2616 cycles & 19,316 cyles & 3114 cyles & 858 cycles \\ 
    \hline
    \end{tabular}
\end{center}

\section{Memory Operations}

\subsection{Main Memory}

\subsubsection{Methodology}

One of the main challenges about reading from main memory is trying to figure out how to force the process to actually read from main memory.  Since a CPU cache exists, the kernel will attempt to read data into it that it think might be referenced later.  For instance, the first memory operation to be estimated and measured is reading a series of integers from memory.  Creating an array on the heap might bring it into cache upon allocation.  If not initially, it will most assuredly bring it to cache after the first reference.  Along with this value, it could potentially bring its neighboring addresses in too because of locality of reference.  So, another reference to this same spot in memory in the next hundred or so loops will read the value from cache instead of main memory.

So these circumstances pose a couple of problems and they need to be dealt with to accurately measure the different memory entitites.  First of all, when trying to measure main memory, the data structure needs to be of significant size that would make it cumbersome to load the entire thing into the cache.  And second of all, there must be a means of flushing the cache between each measurement to ensure that the next reference will be from memory.

The caches that this system has are 32 KB for L1, 256 KB for L2 and 15 MB for L3.  Thus, I first start by creating a 1 GB static array before any measurements take place.  Having a block of memory this big will give me a greater chance of having the data referenced from main memory.

%To randomly index into the array, I utilize the \textit{rand()} function with the current time as the seed.  Each time I access the random function, I bit XOR it with the previous random number time in order for the pseudo-random number to be based on time\textsubscript{n} and time\textsubscript{n-1}.  So for each read timing, the software randomly calculates an integer offset between 0 and 7,999,996.  The calculations for pseudo-randomness is not part of either the estimation or the final measurement.

%After allocating memory and generating a random index, the first \textbf{RDTSCP} value is read.  Then, the four-byte integer is retrieved from a pseudo-random index in the array.  Then, the \textbf{RDTSCP} is read again.  After this, the memory is freed and the cache is cleared by iterating over a large array to hopefully clear out any lingering data.

After allocating memory, the program steps through array sizes from 2\textsuperscript{8} to 2\textsuperscript{29} array sizes.  Though, one cannot just use a for loop to iterate through the elements since the CPU will leverage spacial locality and bring much of the data in memory.  One needs to stride through the different array sizes.  I utilizted different stride values from 256 to 8192 bytes.  There is a section after the main memory and cache sections that go into the results of striding.

After striding through each, array, the difference is calculated and the measurement is performed 1000 times to calculate an average and other interesting metrics.  One of these metrics is the back-to-back load latency.  Larry McEvoy and Carl Staelin define back-to-back load latency as follows:
\begin{addmargin}[1em]{2em}% 1em left, 2em right
Back-to-back-load latency is the time that each load takes, assuming that the instructions before and after are also cache-missing loads.\footnote{McVoy, L., and Staelin, C.  "lmbench: Portable Tools for Performance Analysis" Proceedings of the USENIX 1996 Annual Technical Conference, January 1996, p. 8.}
\end{addmargin}

To measure the load, I followed the example in the aforementioned paper and created a simple linked list of 1000 nodes.  In the \textit{MeasureBackToBackLoad()} function, the timer is first read and the 1000 item linked list is traversed from begining to end.  After the entire list was traversed, the timer was stopped, and the cache was cleared in the same way as the \textit{MeasureMainMemory()} function.

\subsubsection{Estimation}

As far as estimating both memory latency and back-to-back load latency, the costs will be shared amongst both hardware and software, but the hardware cost will dominate.

In the case of memory latency, the instruction to read the integers from memory will cause the caches to be checked first.  There will hopefully be misses at all three levels, and the data will be fetched from main memory and will most likely be put into L1 cache.

The actual code to read the memory is quite minimal and maybe only a few clock cycles of indexing into an array.  A check and a miss at L1 cache will probably cost 10 cycles.  A check and a miss at L2 will cost around twice that at 20 cycles.  And a check and a miss at L3 will cost 50 cycles.  After that, the CPU will need to fetch from main memory.  There will be some hardware latency to do virtual address translation, which might take around 20 CPU cycles.  Then the read itself because of waiting to refresh might take around 50 cycles.  The read itself will take 10.  All in all, I estimate the hardware cost to be the following

\begin{center}
\begin{tabular}{ l|l }
  10 cycles & L1 Miss \\
  20 cycles & L2 Miss \\
  50 cycles & L3 Miss \\
  20 cycles & Address Translation \\
  50 cycles & Refreshing Cells \\
  10 cycles & Memory Read \\
  \hline
  160 cycles & Total
\end{tabular}
\end{center}

As far as back to back load is concerned, I think the cost will be about 20 cycles greater since the L1 cache lines might still be in the process of being written in.  Thus, I estimate back-to-back load to be about 180 clock cycles.

The kernel is not really involved in the read from main memory.  It is mainly handled by the CPU and MMU.  However, because of uncertaintanties, I will put a cost of 10 clock cycles for the kernel.

The two estimates are:

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Type & Hardware Estimate & Software Estimate & Total Estimate & Average Measurement \\      \hline
    Memory Latency & 160 cycles & 10 cycles & 170 cyles & 206.2 cycles (103 ns) \\ 
    Back-to-back Load & 180 cycles & 10 cycles & 190 cycles & 215.0 cycles (108 ns) \\
    \hline
    \end{tabular}
\end{center}


\subsubsection{Measurement}

To access a single four-byte integer from main memory took on average 206 CPU cycles or 103 ns.  It was somewhat pleasing to see that my estimate was within 30\%.  The max was quite higher at 1550 cycles.  This value could be understood as maybe a short context switch.  The measurements are within a reasonable number for accessing a single integer from main memory.


\begin{center}
    \begin{tabular}{ |l | l | l | l | l | l | l |}
    \hline
    Measurement & \# Samples & First Iteration & Min & Max & Mean & Std Dev \\ \hline
    Memory & 1000 it. & 7642 cycles & 178 cycles & 1550 cycles & 206.2 cycles & 248.1  cycles \\ 
    Back-to-back Load & 1000 it. & 269 cycles & 202 cycles & 317 cycles & 215.0 cycles & 15 cycles \\
    \hline
    \end{tabular}
\end{center}

Measuring the back-to-back load latency was an interesting exercise.  My very first implementation went extremely fast.  It was on the order of 10,000 - 20,000 clock cycles to traverse a 1000 node linked list.  After considering it though, it did not seem that difficult to reason.  All the linked list node structure's definition was a pointer to the next node.  Thus, it was an eight byte field that pointed to the next eight byte field.  When I created the list at the beginning of the program, \textit{calloc()} most likely allocated the memory contiguously.  Thus, reading the list front to back allowed for any locality and readahead optimization by the kernel.  So, in a way, the linked list most likely looked like an array in memory.

So, to take another stab at it, I redefined the structure to include a 50 KB data member.  Adding this field gave my linked list structure more weight which provided for more of a random scattering into memory.  After that, I saw more realistic numbers for back-to-back load.  Although, it was not as costly as either the authors of the lmbench paper or I thought it would be.  In fact, it just added on another 5 clock cycles to the main memory measurement.  After subtracting the loop and timings overheads, the average timing was 215 cycles or 108 ns.  The reasoning for the speedup in back-to-back load could potentially be the fact that cache technology and writes are faster in general than from when the paper was written.

\subsection{L1 \& L2 Caches}

\subsubsection{Methodology}

Trying to force a read from L1 cache is somewhat tricky.  There is no guarantee that the variable that one reads will actually be the one in cache.  However, we can get fairly sure.

To read a integer from L1 cache, I start by reading fifty integers from a static array into L1 cache by way of a \textbf{for} loop.  I could not just define an integer in the function since that would put it in a register.  So, fifty integer are read into L1 cache.  Afterwards, I read \textbf{RDTSCP} and then index to the 25th element of the array.  This element should be in L1 cache.  Then, I read \textbf{RDTSCP} again to find how long the read took.

To read an integer from L2 cache was a little more difficult to figure out than L1.  Somehow, the data I would be referencing would have to be forced out of L1 and drop into L2 without going farther into the L3 cache or back to main memory.  The methodology for forcing a value to L2 continutes where L1 left off.

The L1 size for this machine is 32 KB of data.  It also has 32 KB of instruction cache but that does not pertain to this measurement.  The L2 cache is much larger at 256 KB.  To measure L2, data needs to be read into L1 where it would "spill over" into L2 from where I could read.

A large static two-dimensional integer array (5000x1024) was created for this purpose among others.  All in all, this array measures to be around 20 MB. The \textit{MeasureL2Cache} function randomly indexes into this array and reads 50 different 4 KB blocks (or ~ 200 KB) into the L1 and consequently into the L2 cache.  After these 50 blocks are read in, a random block from the first 30 are indexed into again for the integer read.  The reasoning is that the last 20 blocks will more than cover the 32 KB of L1 cache and that hopefully by reading an integer from the first 30, it will be from the L2 cache. 

\subsubsection{Estimation}

Estimating L1 and L2 cache hits and misses is entirely hardware and not kernel specific.  L1 reads are very fast and will be hardly more than the time it takes to read the \textbf{RDTSCP}.  Thus, I estimate that an L1 hit will take three clock cycles.  L2, on the other hand, will take a good bit longer.  And not just twice as long.  There is an L1 cache miss involved and depending on how far away the L2 cache is, I estimate that L2 will rather take five times as long or about 15 clock cycles.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Type & Hardware Estimate & Software Estimate & Total Estimate & Average Measurement \\      \hline
    L1 Cache & 3 cycles & 0 cycles & 3 cyles & 10 cycles (5 ns) \\ 
    L2 Cache & 15 cycles & 0 cycles & 15 cycles & 33.8 cycles (16.5 ns) \\
    \hline
    \end{tabular}
\end{center}

\subsubsection{Measurement}

The measurements for the L1 cache estimation.  The average L1 cache read clocked in at 40 clock cycles or 5 more than the \textbf{RDTSCP} call.   Thus, a L1 read took 2.5 ns.  The standard deviation is 3.05 which gives me confidence for my methodology.

The L2 cache measurement was very close to the estimate as well.  The average L2 cache read was 61.9 clock cycles or about 17 clock cycles more than the \textbf{RDTSCP} read.  This timing includes both the L1 miss and L2 hit.  There was more variance in the results (the standard deviation is 48.86) which is mainly due to the fact that it is a little more difficult to force data into the L2 cache.  My guess is that sometimes the data probably went to L3 cache or back to main memory which explains the max read of 506 clock cycles. 

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l | l |}
    \hline
    Measurement & No. of Samples & First Iteration & Min & Max & Mean & Std Dev \\
    \hline
    L1 Cache & 1000 iterations & 50 cycles & 33 cycles & 50 cycles & 40.84 cycles & 3.05  cycles \\ 
    L2 Cache & 1000 iterations & 57 cycles & 36 cycles & 506 cycles & 61.58 cycles & 48.87  cycles \\ 
    \hline
    \end{tabular}
\end{center}

\subsection{RAM, L1, \& L2 Comparison with Arrays}

As you can see in the below graph, there are steps to indicate both the L1, and L2 caches.  Because of the large amount of cache, we didn't observe a step up until about 2\textsuperscript{23} byte arrays.  This was mainly due to the locality of the striding.  The stride values would often overlap with each other which caused the steps to be seen in values greater than the actual cache values.

Another step up around a 2\textsuperscript{24} array size.  From there, the graph would show L3 and main memory.

\begin{center}
\begin{tikzpicture}[domain=0:3]
\begin{axis}[xlabel={log\textsubscript{2} array size}, ylabel={latency in clock cycles}]
\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class

    10      7.43     0
    11      6.28     0
    12      8.02     0
    13      7.00     0
    14      6.68     0
    15      6.33
    16      6.87
    17      6.28
    18      6.38
    19      6.16
    20      6.13
    21      6.68
    22      6.56
    23      6.74
    24      13.84
    25      21.10
    26      19.26
    27      21.23
    28      24.04
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    10      6.36     1
    11      7.44     1
    12      6.28     1
    13      6.95     1
    14      6.85     1
    15      10.63
    16      6.36
    17      5.86
    18      5.94
    19      6.18
    20      6.16
    21      5.94
    22      6.40
    23      6.54
    24      12.63
    25      19.41
    26      20.69
    27      21.24
    28      25.71
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    10      8.18     2
    11      7.08    2
    12      7.74     2
    13      6.58     2
    14      7.21     2
    15      6.63
    16      6.32
    17      6.64
    18      6.15
    19      6.90
    20      6.26
    21      6.41
    22      6.62
    23      6.84
    24      15.81
    25      20.19
    26      18.72
    27      18.95
    28      23.47
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    10      8.72     3
    11      7.03    3
    12      6.82     3
    13      6.97     3
    14      6.19     3
    15      7.31
    16      6.80
    17      6.51
    18      6.27
    19      5.99
    20      6.56
    21      6.05
    22      6.38
    23      6.54
    24      12.97
    25      18.38
    26      18.75
    27      19.12
    28      23.39
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    10      18.04    4
    11      6.49    4
    12      7.23     4
    13      6.65     4
    14      6.64     4
    15      6.39
    16      6.98
    17      7.30
    18      7.18
    19      6.54
    20      6.34
    21      6.07
    22      6.40
    23      6.49
    24      13.36
    25      18.95
    26      19.51
    27      19.91
    28      24.24
};


\end{axis}
\end{tikzpicture}
\end{center}

\subsection{RAM Bandwidth}

\subsubsection{Methodology}

The first, easy to implement function to read and write from memory would be to take a large array and iterate over it with a for loop.  The single instruction inside the loop would either be the read or the write.  Unfortunately, the loop and the branching would cause significant overhead to the measurement.  Even though, there is a previous measurement of the loop cost, I sought out a more efficient approach.

From James Leiterman's book \textit{32\/64-Bit 80x86 Assembly Language Architecture}, he mentions the \textbf{REP STOSQ} and \textit{REP LODSQ} instructions.  The \textbf{STOSQ} and the \textbf{LODSQ} write and read 8-byte strings of bytes to and from memory, respectively.  The \textbf{REP} prefix informs the CPU to repeatedly perform the instruction for a count specified in the \textbf{RCX} register.  Peforming this instruction will streamline the instructions and remove some of the overhead in branching.

Thus, that is what the \textit{MeasureRamRead()} and \textit{MeasureRamWrite()} functions consist.  The \textbf{RDTSCP} calls wrap both the \textbf{REP STOSQ} and the \textbf{REP LODSQ} instructions.

\subsubsection{Estimation}

The memory has a maximum ideal speed of 1600 MHz.  However, because of the current CPU, the clock for the memory is configured to be 1333 MHz. The data bus is 64 bits wide with some additional bits for error code checking.  There are four memory channels per processor.  However, these channels are shared amongst cores and I don't forsee that the one core that my process is attached to will utilize all four memory channels.

For instance, the Intel website\footnote{http://ark.intel.com/products/64594} specifies the maximum memory bandwidth to be 42.6 GB/s.  And this calculation can be figured out easily.  It takes the fastest DDR3 memory it can handle 1333 MHz, multiplies it by the number of memory channels (4) and then by the 8-byte data bus.

However, these memory channels are shared amongst six cores on the processor.  And since this process will only be on one of the cores, it will probably only use one channel.  So my estimation will only be for one of the channels being utilized.

Thus, we can take the maximum memory bandwidth that Intel provided and divide it by the number of channels (4) since our system is configured with DDR3 1333 MHz RAM.  So, the maximum memory bandwidth for one channel is 10.65 GB/s.

The OS should not add too much overhead, but I will include about a 10\% overhead for it and other things.  So, I hope to reach 9.59 GB/s.

\subsubsection{Measurement}

For RAM read and RAM write, I observed the following measurements:

For the data read operation, it took 5,360,451 clock cycles, or about 2.6 ms, to read from the 5000 x 1024 array on average.  The bandwidth for memory read would then be around:

\begin{gather}
bandwidth = 5000 blocks * 1024 elements * 4 bytes * \frac{2,000,053,000 Hz}{5,360,451 cycles} \\
bandwidth = 7.641 GB/s
\end{gather}

For the data write operation, it took 5,344,615 clock cycles, or about 2.6 ms, to write to the 5000 x 1024 array.  Interestingily enough, this is the exact same time as the data read.  For a sanity check, I ran through the experiment multiple times and I get similar results each time.  The data read and data write are extremely close with one another.  Again the standard deviation is quite small ~ 158,450 clock cycles which is 3\% of the average.  The bandwidth would then be:

\begin{gather}
bandwidth = 5000 blocks * 1024 elements * 4 bytes * \frac{2,000,053,000 Hz}{5,344,615 cycles} \\
bandwidth = 7.664 GB/s
\end{gather}

The write and read bandwidth are quite close with one another.  Unfortunately, the bandwidth is still outside my estimate of 9.64 GB/s.  One reason for lower bandwidth is that this process is bound to a particular core.  If there were a method to leverage all cores and write to memory at the same time, there might be a way to get a more optimized speed.  Also, there are many other services running on this computer.  Some of them are quite large - OpenStack Nova Compute, Swift Object Storage, MySQL, Apach Qpid, and a Windows Server VM.  And unfortunately, I do not have the authority to temporarily suspend these services.  Thus, being within 80\% of my estimate is not difficult to reason.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l | l |}
    \hline
     & \# of Samples & First Iteration & Min & Max & Mean & Std Dev \\
    \hline
    Read & 1000 iterations & 5,383,306 c. & 5,152,504 c. & 7,387,026 c. & 5,360,451 c. & 212,472 c. \\ 
    Write & 1000 iterations & 4,925,780 c. & 5,142,160 c. & 6,415,861 c. & 5,344,615 c. & 198,980 c. \\ 
    \hline
    \end{tabular}
\end{center}

\subsection{Page Fault}

\subsubsection{Methodology}

To measure a page fault on this machine with 128 GB of main memory, I needed to create a file that's considerably large in the file system.  At least the file would need to be large enough for the kernel not to bring the entire thing into memory.  I also wanted to chose a size large enough so that I could seek to random points in the file and almost guarantee a page fault.

Thus, the first thing that I did to prepare for this measurement is to generate a 300 GB file.  \textbf{fallocate} would be a good candidate to make a large file fast.  Unfortunately, this command only allocates block and marks them as unitialized.  Not to mention, I wanted to be sure that the file had true random data in it.

Thus, I use the \textbf{dd} command to generate a 300GB file with data from \textit{/dev/urandom}.  The command took a while to perform - about 16 hours or so!

After a random 300 GB file was in place, I was fairly sure that most of my calls to the \textit{MeasurePageFault()} function would indeed generate a page fault.  After entering the measure function, I open the file and generate a random seek position from 0 to 300 billion.  Next, the timer starts, and both \textit{fseek()} and \textit{fread()} are called.  Then the timer is read again for the final time.  This function is repeated a 1000 times.

\subsubsection{Estimation}

The hard drives that are mounted at /home are configured in a Dell PowerEdge RAID controller, H710P.  The two 1 TB HDDs are configured in a RAID 1 configuration meaning that disk reads can be performed in parallel for greater throughput.  Since this test is only measuring reading a single page fault, the RAID configuration should not benefit this test.

The hardware cost is the most significant cost to this operation.  The cost of accessing main memory has already been measured to be about 100 ns on average.  The cost for reading from an HDD will be exponentially larger - probably more than 10,000 times more!  So the main memory measurement is just noise for this measurement and will not be considered.

The hard drives are 7200 RPM which is 120 RPS, or once every 8.3 ms.  If the platter has to travel at least half that time on average, it will take roughly 4 ms just for the platter to spin to the right position.  Then there is the cost of moving the head and reading the data, which might be another 2 ms.

As far as the OS cost would go, the OS will need to check the page table.  And after not finding it, it will attempt a system call to read from disk.  Like the main memory observation, the OS time to service a page fault will pale in comparison to the hardware cost involved.  However, for measurment's sake, I will estimate the page fault service time for the OS to be 100 ns or equivalent to the cost of reading from main memory.

Thus, the total estimation and average measurement is:


\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Hardware Estimate & Software Estimate & Total Estimate & Average Measurement \\      \hline
    6 ms & 100 ns & ~ 6 ms & 19.8 million cycles (9.9 ms) \\ 
    \hline
    \end{tabular}
\end{center}

\subsubsection{Measurement}

After running the \textit{MeasurePageFault()} routine, I retrieved the following results:

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l |}
    \hline
    \# of Samples & First Iteration & Min & Max & Mean & Std Dev \\
    \hline
    1000 iterations & 33.89 M cycles & 15,603 c. & 87.5 M c. & 19.8 M c. & 10.0 M. c. \\ 
    \hline
    \end{tabular}
\end{center}

My estimate was lower than what was measured and at least within the standard deviation of the average.  There was more variance in the data for a page fault compared to the memory bandwidth.  However, it is still somewhat acceptable.

The standard deviation is half of the average.  The minimum value is 15,603 clock cycles which is a measley 7.8 $\mu$s.  This measurement seems to be too long for it to be accessing main memory since is about 80 times the measurement of main memory.  An explanation for some of the shorter reads is that the head and platter could have been in position with some of its data in a disk cache.  So this miminimum time of 7.8 $\mu$s could be more representative of reading from disk when there is little to no seek time and some of the data could already be buffered to read.

The maximum read was an astounding 43 ms.  A perfect storm of circumstances could cause this phenomenon - a full platter rotate with a context switch to boot.  At any rate, it is a good reminder of the cost of reading from disk and that it can really hinder performance.

Compared to accessing main memory, the average cost of reading a single byte from disk is 4835 clock cycles.  This value was calculated by taking the average time and dividing by the page size - 4096 bytes.  A single byte from main memory is 241 cycles / 4 cycles or ~ 60 cycles.  Thus, a single byte from hard disk is on the order of being 80 times more expensive than one from main memory.  

\section{Network Operations}
\subsection{Round Trip Time}
\subsubsection{Methodology}

Measuring round trip time is an important metric to understand when measuring a system's performance.  With web applications and services becoming more and more popular and sophisticated, optimizing round trip time is essential.  To understand round trip TCP time, I took advantage of the standard TCP Echo Protocol.

The TCP Echo Protocol is conventially on TCP port 7.  Thus, in the \textit{MeasureTcpRoundTrip()} function, the first thing that happens is a socket file descriptor is created by calling the \textit{socket()} function.  Since the TCP protocol is being used, the address family \textbf{AF\_INET} is used, as well as \textbf{SOCK\_STREAM}.

Next, a connection is made using the \textit{connect()} function.  In this function, either the local or remote host is specified along with port 7.  The timer is started after this, and \textit{Send()} is called, which creates the TCP packet with the "Hello World" payload.  The function \textit{recv()} is called next and the timer is stopped after \textit{recv()} returns.

For both local and remote hosts, the \textit{xinetd} (Extended Internet daemon) daemon is utilized for the TCP echo server.  This daemon is a standard internet daemon that implements many of the standard TCP and UDP protocols.  This server merely echoes the message it receives on port 7 back out to the client.

Thankfully, the machine that this gamut of tests has been run on is in a cluster of four other servers that are identical in every way.  Together, they make up a private cloud cluster that runs OpenStack and a host of other services.  So the remote host in question has the exact same hardware and specs as this current one.  Refer to Section 2 for both the local and remote host specs.  For convience, the network card is a 1 Gigabit, full-duplex card.  These machines are connected on Cat5 cable and are configured for 1000BASE-T speeds.

For the ping request comparison, I used the \textbf{ping} utility included in Linux.  For knowing the exact size of the TCP packet, I utilized the \textbf{tcpdump} utility.

\subsubsection{Estimation}

The TCP packet itself will most likely be around 100 bytes with headers, payload, and checksums for TCP and its underlying layers.  For loopback, the hardware cost is taking the data, transferring it internally in the network interface card and receiving it back.  The hardware cost should be much smaller than the remote host.  Thus, the hardware cost of loopback is probably on the order of 10 us.

The hardware cost for remote host will be much greater, because it will send the data onto the ethernet line and later receive it back over the line.  But the hardware cost is double because there are now two network interfaces - the local and the remote and the transfer over the wire.  Since this host is adjacent to the local machine, this is neglible.  And since these machines are on a private cluster, there will not be much network traffic that will provide collisions.  However, I will triple my estimate of 10 us for loopback to 30 us for remote.

As far as the software overhead, there is the cost of writing to the socket.  The data is sent from the application to a kernel buffer and then transferred out the network interface.  There is also some uncertainty on when the kernel will send the data out of its buffer.  Thus, transferring the data from memory may take about 100 us and another 100 us of random delay inside the kernel since the time to send is indeterminate.

For loopback, the data will then work its way back into a kernel buffer and back to the application which is double the send cost.  Thus, the software cost will be about 50 us for loopback.  For remote, the cost will be at least double.  Again, I will triple this value which will put it at 150 us for remote host.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Host & Hardware Estimate & Software Estimate & Total Estimate & Average Measurement \\      \hline
    Local & 10 us & 50 us & 60 us & 49,526 cycles (24.7 us) \\ 
    Remote & 30 us & 150 us & 180 us & 203,726 cycles (102 us) \\
    \hline
    \end{tabular}
\end{center}

\subsubsection{Measurement}

The increase of network speed from local to remote was expected.  I had originally thought that the remote round trip would be about triple the loopback.  In this case, the remote round trip was about quadruple that of loopback.

Remote ideally should be a little over double of what loopback is since it is pretty much doing everything loopback is doing, except doing it twice.  However, remote hosts add much more complexity such as network traffic and different machine workloads and configuration.  In this configuration, the machines are pretty much identical.  However, all too often this is not the case.

Loopback is 24 us which is 38 times the ideal 0.64 us latency for a 64 byte TCP packet.  This increase shows that there is some significant overhead in how data is transferred from the application to the kernel and when the kernel actually send it out through the network interface.  It is not immeadiate and I think these latencies prove that theory.  For the remote host, there are now two kernels and four movements of data traveling up and down the network stack.  Mixed with collisions and network uncertainty, the measured values seem realistic.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l | l |}
    \hline
    Measurement & \# of Samples & First Iteration & Min & Max & Mean & Std Dev \\
    \hline
    Local & 100 iterations & 25 us & 18.5 us & 25.9 us & 24.7 us & 2.1 us \\ 
    Remote & 100 iterations & 101 us & 99.2 us & 104 us & 101.9 us & 1.9 us \\
    ping local & 10 iterations & 35 us & 15 us & 35 & 22 us & 8 us \\
    ping remote & 10 iterations & 228 us & 158 us & 228 us & 197 us & 12 us \\
    \hline
    \end{tabular}
\end{center}

Comparing the \textbf{ping} request is interesting.  \textbf{ping} was faster for loopback.  And again, this is expected because the \textbf{ping} command is a system call into the kernel.  And the kernel sends an \textbf{ICMP} packet to the IP.  The results are faster for loopback.

However, the remote TCP measurements were actually faster than what \textbf{ping} reported for remote.  This may be for a couple reasons.  For one, the TCP packets were sent 1000 at a time and then averaged.  Some of this throughput could have achieved a greater bandwidth over time compared to \textbf{ping} which might be more discrete with more initialization and teardown with each request.  I did notice that if only one TCP request was sent instead of 1000, the first one always took exceedingily longer than the rest of the packets.  It is for this reason why there might be some overhead in the \textbf{utility} for remote requests.

\subsection{Peak Bandwidth}

\subsubsection{Methodology}

To measure peak bandwidth, the setup is identical to the round trip study.  Sockets are used for establishing a TCP connection.  One thousand packets are sent in a row with the timing starting right before and right after the last message is received.  Full duplex is measured for peak bandwidth.

The biggest aspect that I needed to change for measuring peak bandwidth was the payload size.  For round trip, the payload was small - only 11 bytes.  This time, the plan was to maximize the MTU size which is 1500 bytes for Ethernet.  Thus, the TCP payload size was set at 1400 which gave 100 bytes for all the different headers.

To attempt peak bandwidth, 1000 packets were sent contiguously.  This test was run 100 times to calculate an average and other metrics.  Again, the TCP Echo protocol is utilized.  Thus, for local and remote, \textbf{xinetd} is the echo server that responds to the TCP packets.

\subsubsection{Estimation}

Since peak bandwidth is tied closely to round trip time, I will use the value that was measured in round trip to inform the estimate here.  It took 24.7 us to send 64 bytes through loopback.  Or, in other words, it took about 385 ns to send and receive one byte.  This is equivalent to about 2.6 MB/s.

Now if the payload is increased, there should be more throughput between the application and the kernel and the kernel and the network interface card.  Thus, there should hopefully be at least a 50\% improvement in data throughput.  Thus, the estimate for peak throughput should be around 1 byte every 190 ns or about 50 MBit.

\subsubsection{Measurement}

Using \textbf{tcpdump}, I was able to ascertain that the payload and the headers equaled 1477 bytes.  Thus, the total amount of data that is sent is 

\begin{gather}
total_size = 1477 bytes * 1000 iterations * 10 bits (for framing) \\
total_size = 14,770,000 bits/second
\end{gather}

It took on average 173,179,873 clock cycles to send this amount of data through loopback, or 86.5 ms.  Thus, the bandwidth for localhost is 170 MBit/s.

For the remote host, it took on average 418,700,386 clock cycles to send this amount of data or 209 ms.  Thus, the bandwidth for the remote host is 70 MBit/s.

Again, these are disappointing results.  These speeds are nowhere near the theoretical limit of 1 Gbit/second.  There are a lot of reasons why this is though.  Running \textbf{tcpdump} during one of the trials showed a bunch of resends for the packets.  These are some of the packets that are not accounted for in this measurement.  Also there could be network collisions happening.  Not to mention, the current machine is a head node for a group of five compute nodes.  Thus, it has much network activity coordinating different virtual machine and compute efforts.  So these TCP packets were competing heavily with others.

Another interesting note is that the maximum bandwidth for loopback was over 300 Mb/s, which is almost twice the average.  I think it is telling about the network load of this particular system.

The bandwidth results are the following:

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l | l |}
    \hline
    Measurement & \# of Samples & First Iteration & Min & Max & Mean & Std Dev \\
    \hline
    Local & 100 iterations & 301.4 Mb/s & 112.8 Mb/s & 301.4 Mb/s & 170 Mb/s us & 13.2 Mb/s \\ 
    Remote & 100 iterations & 69 MB/s & 66 MB/s & 73 Mb/s & 70 Mb/s & 9.8 Mb/s \\
    \hline
    \end{tabular}
\end{center}

\subsection{Connection Overhead}

\subsubsection{Methodology}

Measuring the TCP connection establishment is the act of measuring the two methods \textit{socket()} and \textit{connect()}.  The first function merely creates the socket and returns the file descriptor for the socket.  The latter function actually creates the TCP connection.  This TCP handshake is the operation of the client machine sending a SYN TCP packet.  The server responds with a SYN-ACK packet.  And finally the client sends an ACK packet which establishes the TCP connection.

This handshaking was done in the previous TCP functions, but they were just not measured.  Since the TCP establishment is just a factor of these two functions, the timer starts before the \textit{socket()} function and the timer is stopped right after the \textit{connect()} function.

For teardown, TCP termination is a four-way handshake.  Tearing down a TCP connection involves calling the \textit{shutdown()} function and \textit{close()}.  The \textit{shutdown()} function shuts down send and recieve functions for the associated socket.  And \textit{close()} actually closes the file descriptor associated with the socket.

For teardown measurement, the start timer and end timer are wrapper around the \textit{shutdown()} and \textit{close()} functions.  For both establishment and teardown, the measurement is repeated 1000 times and then averaged.  As usual, other metrics such as min, max, and standard deviation are recorded as well.

\subsubsection{Estimation}

Again, the measurement for the TCP round trip is helpful here.  The average round trip measurement for a single TCP packet is about 25 us for localhost and 102 us for remote.  I imagine that the connection overhead will be a bit over this because there is about two round trips for handshaking.

So I imagine that the total localhost connection would be a little under twice the round trip speed, which would probably be about 40 us.  For remote host, I imagine that it is also a little under twice the round trip which would be about 180 us.  

For localhost, I would think that the majority of this speed would be software with hardware bearing a neglibile cost.  So, I will split the estimate 10\%/90\% as far as hardware software go.  The hardware cost is more for remote host which would probably be more like 20\%/80\% since there is more hardware involved and the data goes over the physical network.  For teardown, I expect the same ratios.

I expect that initialization will take a little longer than teardown.  Some of this will be accounted as software overhead in the kernel for initializing memory for the new TCP connection.  Thus, the software cost for TCP setup will be a little longer than teardown.  I imagine that it will be about 10 us for initialization and maybe three times that for remote.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Host & Hardware Estimate & Software Estimate & Total Estimate & Average Measurement \\      \hline
    Local Setup & 4 us & 46 us & 50 us & 71,284 cycles (35.6 us) \\ 
    Local Teardown & 4 us & 36 us & 40 us & 26,153 cycles (13.1 us) \\ 
    Remote Setup & 36 us & 174 us & 210 us & 436,866 cycles (218 us) \\
    Remote Teardown & 36 us & 144 us & 180 us & 19,677 cycles (9.8 us) \\
    \hline
    \end{tabular}
\end{center}

\subsubsection{Measurement}

My estimates for TCP setup were actually very close to what the average was.  Unfortunately, the estimates for teardown were too conservative.  Part of making a good estimate is having previous information.  And having previous information about TCP round trip time really helped to estimate TCP initialization.

\begin{center}
    \begin{tabular}{ | l | l | l | l | l | l | l |}
    \hline
    Measurement & \# of Samples & First Iteration & Min & Max & Mean & Std Dev \\
    \hline
    Local Setup & 100 iterations & 81,600 c & 65,837 c. & 80,103 c. & 71,284 c. & 5301 c. \\ 
    Local Teardown & 100 iterations & 22,622 c. & 19,559 c. & 61,590 c. & 26,153 c. & 11,903 c. \\
    Remote Setup & 100 iterations & 1,052,760 c. & 232,219 c. & 535,375 c. & 436,866 c. & 222,479 c. \\ 
    Remote Teardown & 100 iterations & 12,412 c. & 10,563 c. & 56,149 c. & 19,677 c. & 13,208 c. \\
    \hline
    \end{tabular}
\end{center}

The average time for localhost connection initialization is 71,284 cycles or 36 ms.  This estimate is within 14 ms of my average which is fairly good.  The average time for local teardown is 26,153 cycles or 13 ms.  This teardown speed is more than half of what I expected. 

The average time for remote connection initialization is 436,866 cycles or 218 us.  This average was extremely close to my estimate of 210 us.  However, the remote teardown was extremely faster and not what I had expected.  The remote teardown took a measely 19,677 cycles on average which is just under 10 us.  The timing for teardown was actually less than what was measured locally.

Being close to the estimates for TCP intialization and far from the estimates for teardown make me think that I understood more of the underlying mechanics of initialization and not of teardown.  For teardown, the very quick speed might indicate that the TCP packets are sent asynchronously instead of synchronously.  For instance, \textit{connect()} must be synchronous so the caller can be sure that it can use the socket.  However, \textit{close()} doesn't necessarily have to be synchronous and that is why the timing could be a lot shorter than anticipated.

\section{File System}

\subsection{File Cache}

\subsubsection{Methodology}

Reading from the file cache buffer means using a lot of the standard file functions included in \textbf{stdio}.  The 300 GB file from the page fault section is used as the file to read into.  The goal of this section is take different sizes of it and measure the read time.

The first call that \textit{MeasureFileCache()} does is open the file with \textit{fopen()}.  After opening the file, the code reads in the file depending on the filesize.  The whole file is read in at once with \textit{fread()}.  File sizes from 1 MB to 1 GB are tested in powers of 2.  The timing functions are wrapped around the \textit{fread()} call.

What this function should do is to test the limits of the file cache.  If the file cache is too small, there should be a jump in the read latency.

The measurement is repeated a 100 times and various metrics are calculated.

\subsubsection{Estimation}

For the first read of the data, there definitely could be some latency involved because it would mean that Linux would be reading the file from disk into the file cache.  However, after the first read, the data would be cached in memory.  Every subsequent read should be a good bit faster.

The hardware cost would mainly be memory and how Linux handles the file cache itself.  For the first read, I would imagine that the 7200 RPM RAID 1 drive read would be fairly fast.  It would probably be on the magnitude of 200 us.  As the file size would grow by 2, I would imagine that the read time would grow by 2 as well.  Since this machine has a large amount of memory ~ 128 GB, I don't think that a 1 GB file will grow beyond the file cache.  There will probably be some small jump, but nothing like a step.

On the software end, these functions will merely be reads from main memory.  As we observed in the main memory section, a read of this magnitude will probably be about 100 us.  Thus, the total estimate is probably around 300 us for a 1 MB file, with the cost increasing by 2 as the file size increases.

\begin{center}
    \begin{tabular}{ | l | l | l | l |}
    \hline
    Hardware Estimate & Software Estimate & Total Estimate \\ 
    200 us & 100 us & 300 us \\ 
    \hline
    \end{tabular}
\end{center}


\subsubsection{Measurement}

As one can observe in the graph, the large amount of memory on the system has paid well.  As the file size grew by two, so did the file read time.  One can compare the estimate (red) to the measured (blue).  The first value set off the trend.  The measured speed of reading a 1 MB file from the file cache was a little over 200 ms.  Since the system has 128 GB of memory, it would probably take an extremely large file for the buffer cache to be overrun.


\begin{center}
\begin{tikzpicture}[domain=0:3]
\begin{axis}[xlabel={log\textsubscript{2} file size}, ylabel={time in ms}]

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class

    20      0.211     0
    21      0.582     0
    22      1.11     0
    23      2.3     0
    24      4.87     0
    25      13.3     0
    26      26.7     0
    27      58.2     0
    28      121.4    0
    29      239.5    0
    30      482.4    0
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    20      0.3    1
    21      0.6    1
    22      1.2     1
    23      2.4     1
    24      4.8     1
    25      9.6
    26      19.2
    27      38.4
    28      76.8
    29      153.6
    30      307.2
};

\end{axis}
\end{tikzpicture}
\end{center}

\subsection{File Read}

\subsubsection{Methodology}

For file read, the goal is to measure access to disk.  Using the file stream functions like \textit{fopen()} and \textit{fread()} will not do because these use the Linux file buffer cache.  Thus, the raw functions, such as \textit{open()} and \textit{read()} must be used so the buffer cache will not be used.

The implementation of the \textit{MeasureFileRead()} is fairly straightforward.  A large 300 GB file was created when the page fault measurement was performed.  The \textit{MeasureFileRead()} function merely reads this file in various file sizes.  It starts with a file size of 1 MB and scales by a factor of 2 until a filesize of 1 GB is reached.

So it starts by creating a block of memory for the amount of data it will read in.  For instance, the first test allocates a 1 MB block of memory.  After creating this block of memory, the function opens the file using the raw interface \textit{open()}.  After retrieving the file descriptor, the function starts the timer.  Next it reads in the block using \textit{read()}.  After the function call, it stops the timer and returns the result.  This measurement is performed 100 times and is averaged.

For random access, the measurement is really similar to contiguous access except for the fact that it is randomly indexed into the file for 1 KB at a time.  Randomly indexing into the file will cause the seek and read time to be longer.

\subsubsection{Estimation}

The speed of this function will be based mostly on hardware which is the hard-drive speed with its seek time.  The hard drive is 7200 RPM, connected through eSATA which has a theoretical top speed of 6 GB/s.  It is also configured in a RAID 1 configuration which should give high throughput. Thus, if 80\% of the eSATA data throughput could be achieved, the rate would be 4.8 GB/s.  So to read a 1 MB file, the hardware cost should be about 218 us.  A 2 MB file's hardware cost should take twice this long ~ 436 us and so on.

The software cost is small compared to the hardware cost.  It will be responsible for transferring the data from disk to program memory.  An extra 100 us will be added on top of each hardware cost.  Thus, a 1 MB file would take 318 us.  A 2 MB would becaome 536 us and so on.  So a 1 GB file should take around 325 ms, reading it contiguously.

However, for random reads, there will be much more seek time.  For each file, the read speed will most likely at least double because of a random seek for each 1 KB block.  Sometimes the disk head will be close by, but other times it might jump around.  This will especially be true for large files.

The following graph shows the speed estimate for contiguous reads and random reads for files from 1 MB to 1 GB, scaled by a factor of 2.

\subsubsection{Measurement}

The sequential reads were somewhat on par with my estimates.  The sequential reads can be seen in blue.  The estimate for the sequential reads can be found in orange.  The first read is what determines the rest of the graph.  In this case, the 1 MB file took about 400 us to read while the estimate was at 318 us.  So the entire estimate lived underneath the actual measurements.  One can observe that this graph models what happened for the file buffer cache.  The two grew as a power of 2.

The random read results surprised me.  The random reads took longer than expected.  The interesting thing to note is that the factor of 2 stayed consistent as the file size changed.  I imagined that as the files grew larger, the random reads would cause the disk head to seek more which would result in much longer read times because of the seek.  However, that didn't seem to be the case.  The mere fact though that it is a random read is evident in the starting point of the graph, since it starts at 2 ms.  In constract to 409 us, it is almost an order of magnitude larger than sequential.

It should be mentioned, however, that the read isn't necessarily sequential.  Depending on how full the disk was when the file was created, the file blocks could be spread around the disk.  If any defragmenting has happened, that would have ensured greater locality.  My guess is that the file has fairly good locality because the reads were moderately fast and grew at a known, steady rate. 


\begin{center}
\begin{tikzpicture}[domain=0:3]
\begin{axis}[xlabel={log\textsubscript{2} file size}, ylabel={time in ms}]

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class

    20      0.409     0
    21      0.833     0
    22      1.634     0
    23      3.45       0
    24      7.38      0
    25      15.6     0
    26      30.6      0
    27      61.1      0
    28      121.5     0
    29      240.8     0
    30      484.1     0
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    20      2.08    1
    21      4.18    1
    22      8.48     1
    23      16.9     1
    24      40.0     1
    25      68.0
    26      136.2
    27      272.9
    28      546.6
    29      1100.9
    30      2204.9
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    20      0.318    2
    21      0.636    2
    22      1.272     2
    23      2.54     2
    24      5.09     2
    25      10.2
    26      20.4
    27      40.7
    28      81.4
    29      162.8
    30      325.6
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    20      0.63    3
    21      1.26    3
    22      2.52     3
    23      4.04     3
    24      8.08     3
    25      16.16
    26      32.32
    27      64.64
    28      128.128
    29      256
    30      512
};

\end{axis}
\end{tikzpicture}
\end{center}



\subsection{Remote File Read}

\subsubsection{Methodology}

The implmentation for reading remote files is exactly the same as local files, except for the fact that the file being read is on the network.  The network file system is NFS and served by a high throughput Isilon storage device.  The implmentation is exactly the same as a local file so please refer to the previous section to understand the methodology.

\subsubsection{Estimation}

The Isilon device that is serving the file has a theoretical throughput to max out the Gigabit network connection.  So, I will base my estimation on a high percentage of the gigabit network connection.

Because of the maximum MTU size of 1500 bytes, the 1 MB file will be broken up into hundreds and thousands of TCP packets.  The payload of the packet will be 1400 bytes.  So there will be overhead for each packet because of network headers.  A 1 MB file would then be broken up into about 750 packets.  Using the network speed in the previous network section, the cost to send about 1000 packets was about 24 ms.  So, a 1 MB file would probably be about three quarters of this which is 18 ms.  A 1 GB file would be about 1.8 s.

Randomly seeking into a file over the network will add a cost similar to the local file.

FILLIN FILLIN

\subsubsection{Measurement}

Out of all the measurements in this paper, this measurement surprised me the most.  I wouldn't have thought that a remote network file read would be faster than reading from disk.  However, the measurements proved me wrong.  And I know that it is not an implementation mistake because both the local read and the network read utilize the same code.

To start with, I thought that the network read would start at 18 ms.  However, the network read for the 1 MB file took only 119 us!  And just like the other file read measurements, the file read time had a nice linear growth as the file size increased.  One of the main reasons for this increase in speed is the fact that the network file system is being served by an Isilon file storage device.  These devices cost on the order of hundreds of thousands of dollars.  Their main selling goal is that they can serve data fast to consumers.  And since the Isilon drive is hooked up with gigabit ethernet, the speed is actually very tremendous.  This network storage device's goal is to serve large genome files and high resolution images for the employees at Illumina.

Again, the random read was higher than the continuous one.  The order of magnitude for a random read was a little over 10 times the continuous read.  Isilon storage devices are probably tuned for streaming, continuous reads.

Below, the red is the estimate with the yellow being the random access read and the blue being the continuous read.


\begin{center}
\begin{tikzpicture}[domain=0:3]
\begin{axis}[xlabel={log\textsubscript{2} file size}, ylabel={time in ms}]

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class

    20      0.119     0
    21      0.232     0
    22      0.465     0
    23      0.935       0
    24      1.86      0
    25      3.71     0
    26      7.43      0
    27      14.8      0
    28      29.7     0
    29      59.4     0
    30      119.0     0
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    20      1.79    1
    21      3.59    1
    22      7.22     1
    23      14.3     1
    24      28.5     1
    25      57.1
    26      114.2
    27      228.1
    28      456.1
    29      912.4
    30      1824.4
};

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class
    20      18    2
    21      36    2
    22      72     2
    23      144     2
    24      288     2
    25      576
    26      1152
    27      2304
    28      4608
    29      9216
    30      18432
};



\end{axis}
\end{tikzpicture}
\end{center}


\subsection{Contention}

\subsubsection{Methodology}

This section describes the methodology for measuring a different number of threads to read blocks from different files on the local system.  The first measurement is just one thread reading one kilobyte block from one file.  It is then scaled to two threads, three threads, and all the way up to twenty threads.

Threads are created with \textit{pthread\_create()}.  In the thread function, the function opens a file with the standard \textit{open()} function like the previous sections.  It reads one kilobyte of data and returns.  However, before any of the threads read a block from a file, all the threads need to be synched up.  A barrier is used.  

Thus, \textit{pthread\_barrier\_init()} is called initially to create a pthread barrier.  And \textit{pthread\_barrier\_wait()} is called within the thread function.  This function will sync up all the threads.  Thus, each file is opened before the barrier.  The barrier is then enforced.  After the threads are synced up and clear the barrier, the timer is started.  After the timer is started, each one kilobyte file is read and the file is closed and the thread returns.  In the parent function, it performs a \textit{pthread\_join()} on each thread to guarantee that each thread returns.  After the join, a time is recorded.  To get the total time, it takes the minimum time from all the threads and subtracts it by the latest time.

\subsubsection{Estimation}

When using one, two, three, or four threads, I don't imagine contention to be much of an issue.  However, as the threads increase, I imagine that there will be more of a hardware cost for the hard drive since there will be more seeking involved when reading the different files.

To read a single block from a file for one thread will only be about 20 us.  With each additional thread, I imagine that the hardware cost will triple.  As far as the software cost goes, it will be mostly consistent with the number of threads.  The software will mainly be responsbile for copying data from disk to memory.  Starting at one thread which is about 20 us, the graph below shows how contention will grow as more threads are added.

\subsubsection{Measurement}

With one thread, the read took 128 us.  This was a good bit more than my estimate of 50 us.  Some of that extra cost included some of the cost of joining the thread.  As each thread was added, a constant offset seemed to apply to the time.  For the most part, this value did not vary and is on the order of 100-300 us.

As the number of threads increased, I imagined that the graph would grow more exponential as the threads tried to read from its portion of the disk.  There would be a heavy seek time as the head would bounce around from thread to thread.  Some of the graph might show that the different files that the threads are allocating have spatial locality with one another.

\begin{center}
\begin{tikzpicture}[domain=0:3]
\begin{axis}[xlabel={\# of threads}, ylabel={time in ms}]

\addplot[scatter, scatter src=\thisrow{class},
      error bars/.cd, y dir=both, x dir=both, y explicit, x explicit, error bar style={color=mapped color}]
      table[x=x,y=y] {
    x       y      class

    1      0.128     0
    %2      0.242     0
    %3      0.382     0
    %4      0.505       0
    5      0.637      0
    %6      0.978     0
    %7      1.25      0
    %8      1.62      0
    %9      2.24     0
    10      2.66     0
    %11      3.04     0
    %12      3.54     0
    %13      3.83     0
    %14      4.18     0
    15      4.50     0
    %16      4.98     0
    %17      5.35     0
    %18      5.80     0
    %19      6.00     0
    20      6.19     0
    25      8.47     0
    30      10.4
    35      12.5
    40      14.5
};





\end{axis}
\end{tikzpicture}
\end{center}

\newpage
\section{Final Results}

Here is a summary of all the estimations and measurements above:

\begin{center}
    \begin{tabular}{ | l | l | l | l | l |}
    \hline
    Operation & Base HW & Est. SW Overhead & Predicted Time & Measured Time \\ \hline
    RDTSCP & 24 c & 5 cycles & 30 c & 35 c (18 ns) \\ 
    Loop & 33 cycles & 5 cycles & 38 c & 12 c (6 ns) \\ 
    Function 0 & 37 c & 5 cycles & 43 c & 37 c (24 ns) \\
    Function 1 & 39 c & 5 cycles & 44 c & 38 c (24 ns) \\ 
    Function 2 & 41 c & 5 cycles & 46 c & 40 c (24 ns) \\ 
    Function 3 & 43 c & 5 cycles & 48 c & 40 c (24 ns) \\ 
    Function 4 & 45 c & 5 cycles & 50 c & 41 c (24 ns) \\
    Function 5 & 47 c & 5 cycles & 52 c & 40 c (24 ns) \\ 
    Function 6 & 49 c & 5 cycles & 54 c & 42 c (24 ns) \\ 
    System Call & 37 c & 100 c & 137 c & 147 c (74 ns) \\ 
    Process Creation & 50,000 c & 20,000 c & 70,000 c & 277,758 c (139 us) \\ 
    Thread Creation & 10,000 c & 10,000 c & 20,000 c & 35,443 c (18 us) \\
    Process Context Switch & 140,000 c & 10,000 c & 150,000 c & 82,674 c (41 us) \\ 
    Thread Context Switch & 35,000 c & 10,000 c & 45,000 cycles & 3114 c (1.6 us) \\
    Memory Latency & 160 cycles & 10 c & 170 cyles & 206.2 cycles (103 ns) \\ 
    Back-to-back Load & 180 c & 10 cycles & 190 cycles & 215.0 c (108 ns) \\
    L1 Cache & 3 cycles & 0 c & 3 cyles & 10 c (5 ns) \\ 
    L2 Cache & 15 cycles & 0 c & 15 c & 33.8 c (16.5 ns) \\
    Memory Bandwidth &  10.65 GB/s  & -1.07 GB/s & 9.59 GBs & 7.664 GB/s \\
    Page Fault & 6 ms & 100 ns & ~ 6 ms & 19.8 mil c (9.9 ms) \\ 
    Local Round Trip & 10 us & 50 us & 60 us & 49,526 cycles (24.7 us) \\ 
    Remote Round Trip & 30 us & 150 us & 180 us & 203,726 cycles (102 us) \\
    Local Peak Bandwidth & 50 Mb/s & 0 Mb/s & 50 Mb/s & 13.2 Mb/s \\
    Remote Peak Bandwidth & 30 Mb/s & 0 Mb/s & 30 Mb/s & 9.8 Mb/s \\
    Local Setup & 4 us & 46 us & 50 us & 71,284 cycles (35.6 us) \\ 
    Local Teardown & 4 us & 36 us & 40 us & 26,153 cycles (13.1 us) \\ 
    Remote Setup & 36 us & 174 us & 210 us & 436,866 cycles (218 us) \\
    Remote Teardown & 36 us & 144 us & 180 us & 19,677 cycles (9.8 us) \\
    File Cache (1 MB) & 200 us & 100 us & 300 us & 211 us \\ 
    Loc. File Seq. Read (1MB) & 218 us & 100 us & 318 us & 409 us \\ 
    Loc. File Ran. Read (1MB) & 436 us & 100 us & 536 us & 2.08 ms \\ 
    Rem. File Seq. Read (1MB) & 18 ms & 100 us & 18.1 ms & 119 us \\ 
    Rem. File Ran. Read (1MB) & 36 ms & 100 us & 36.1 ms & 1.79 ms \\ 
    Contention (10 threads) & 1 ms & 200 us & 1.2 ms & 2.66 ms \\
    \hline
    \end{tabular}
\end{center}



%%% End document
\end{document}